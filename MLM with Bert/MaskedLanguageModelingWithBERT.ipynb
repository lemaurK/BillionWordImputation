{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b125a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 06:26:58.764908: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 06:26:59.299391: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-14 06:26:59.339126: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-14 06:26:59.339170: I tensorflow/tsl/cuda/cudart_stub.cc:28] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-14 06:26:59.633355: E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay\n",
      "2022-12-14 06:27:00.947496: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-14 06:27:00.947661: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-14 06:27:00.947668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0d22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5983951",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"billion-word-imputation/train_v2.txt/train_v2xa\", mode='r') as file:\n",
    "    data = file.read()\n",
    "    train = data.splitlines()\n",
    "    \n",
    "with open(\"billion-word-imputation/test_v2.txt/test_v2xa\", mode='r') as file:\n",
    "    data = file.read()\n",
    "    test = data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2086b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2001/2148595058.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = train_df.append(test_df)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d33a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 06:27:02.878225: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-14 06:27:02.878346: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-14 06:27:02.878668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LeMaursPC): /proc/driver/nvidia/version does not exist\n",
      "2022-12-14 06:27:02.880598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(all_data.values.tolist(),config.VOCAB_SIZE,config.MAX_LEN,special_tokens=[\"[mask]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8105116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9f9ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[inp_mask_2mask] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(3, mask_token_id, inp_mask_2random.sum()    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34f81757",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = encode(train_df.values)  # encode reviews with vectorizer\n",
    "y_train = train_df.values\n",
    "train_classifier_ds = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(config.BATCH_SIZE))\n",
    "\n",
    "x_test = encode(test_df.values)\n",
    "y_test = test_df.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(config.BATCH_SIZE)\n",
    "\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((test_df.values, y_test)).batch(config.BATCH_SIZE)\n",
    "\n",
    "x_all_review = encode(all_data.values)\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(x_all_review)\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_train, y_masked_labels, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37e0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2961640",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719dbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45378720",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a05bd5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c59287af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 256, 128)     3840000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 256, 128)    0           ['word_embedding[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattention (  (None, 256, 128)    66048       ['tf.__operators__.add[0][0]',   \n",
      " MultiHeadAttention)                                              'tf.__operators__.add[0][0]',   \n",
      "                                                                  'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 256, 128)    0           ['tf.__operators__.add[0][0]',   \n",
      " mbda)                                                            'encoder_0/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_1[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)     (None, 256, 128)     33024       ['encoder_0/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 256, 128)    0           ['encoder_0/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_2[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)                (None, 256, 30000)   3870000     ['encoder_0/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,809,584\n",
      "Trainable params: 7,809,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "346c4a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 06:27:04.701530: I tensorflow/core/common_runtime/executor.cc:1195] [/device:CPU:0] Executor start aborting: INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [6250,256]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2022-12-14 06:27:04.702172: I tensorflow/core/common_runtime/executor.cc:1195] [/device:CPU:0] Executor start aborting: INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [6250,256]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 265ms/step loss: 8.31\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was [UNK]',\n",
      " 'probability': 0.040193595}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': '\"',\n",
      " 'prediction': 'i have watched this \" and it was [UNK]',\n",
      " 'probability': 0.038513295}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'i have watched this and and it was [UNK]',\n",
      " 'probability': 0.03482467}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'in',\n",
      " 'prediction': 'i have watched this in and it was [UNK]',\n",
      " 'probability': 0.033292953}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was [UNK]',\n",
      " 'probability': 0.032218978}\n",
      "196/196 [==============================] - 472s 2s/step - loss: 8.3157\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 65ms/step- loss: 7.36\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was [UNK]',\n",
      " 'probability': 0.03655534}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was [UNK]',\n",
      " 'probability': 0.034519564}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'i have watched this and and it was [UNK]',\n",
      " 'probability': 0.03334771}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': '\"',\n",
      " 'prediction': 'i have watched this \" and it was [UNK]',\n",
      " 'probability': 0.032391015}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was [UNK]',\n",
      " 'probability': 0.028775433}\n",
      "196/196 [==============================] - 454s 2s/step - loss: 7.3611\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 58ms/step- loss: 7.27\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': '\"',\n",
      " 'prediction': 'i have watched this \" and it was [UNK]',\n",
      " 'probability': 0.03154233}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was [UNK]',\n",
      " 'probability': 0.02665583}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was [UNK]',\n",
      " 'probability': 0.024935933}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'in',\n",
      " 'prediction': 'i have watched this in and it was [UNK]',\n",
      " 'probability': 0.02289237}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was [UNK]',\n",
      " 'probability': 0.0208753}\n",
      "196/196 [==============================] - 449s 2s/step - loss: 7.2730\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 56ms/step- loss: 7.25\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': '\"',\n",
      " 'prediction': 'i have watched this \" and it was [UNK]',\n",
      " 'probability': 0.036682133}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'i have watched this and and it was [UNK]',\n",
      " 'probability': 0.024566863}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was [UNK]',\n",
      " 'probability': 0.023501497}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was [UNK]',\n",
      " 'probability': 0.020462157}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'in',\n",
      " 'prediction': 'i have watched this in and it was [UNK]',\n",
      " 'probability': 0.018361425}\n",
      "196/196 [==============================] - 461s 2s/step - loss: 7.2502\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 56ms/step- loss: 7.24\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': '\"',\n",
      " 'prediction': 'i have watched this \" and it was [UNK]',\n",
      " 'probability': 0.06112807}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'i have watched this and and it was [UNK]',\n",
      " 'probability': 0.033630606}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was [UNK]',\n",
      " 'probability': 0.030439883}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'in',\n",
      " 'prediction': 'i have watched this in and it was [UNK]',\n",
      " 'probability': 0.028304411}\n",
      "{'input_text': 'i have watched this [mask] and it was [UNK]',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was [UNK]',\n",
      " 'probability': 0.02787209}\n",
      "196/196 [==============================] - 441s 2s/step - loss: 7.2435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74cad451c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "732d1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_masked_model.save(\"bert_mlm_billion.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
